{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "70c33353-f5ab-43df-a942-393a110f49ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "744ffa30-9603-42df-b920-008385330710",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./mnist_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "97cae09c-640b-447d-ad1b-ee3fd9fa417d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
      "0      7    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "1      2    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "2      1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "3      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "4      4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "\n",
      "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
      "0      0      0      0      0      0      0      0      0  \n",
      "1      0      0      0      0      0      0      0      0  \n",
      "2      0      0      0      0      0      0      0      0  \n",
      "3      0      0      0      0      0      0      0      0  \n",
      "4      0      0      0      0      0      0      0      0  \n",
      "\n",
      "[5 rows x 785 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0c643cde-7c8a-455e-b4d7-15a0e66833e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "labels_col = train_data['label']\n",
    "print(labels_col.shape)\n",
    "print(labels_col[19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "21cc517b-272c-4fde-a9ee-1f91a66bb9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_new = train_data.drop('label', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "29e36b32-d5e9-4983-b5b5-fbfd21fb189b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(train_data_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a29ed4f4-44af-4cb3-a663-193cb4e77cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784,)\n",
      "2\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (4,4))\n",
    "\n",
    "indx = 147\n",
    "\n",
    "matrix_data = train_data_new.iloc[indx].values.reshape(28,28)\n",
    "plt.imshow(matrix_data, interpolation = \"none\", cmap = \"gray\")\n",
    "\n",
    "print(train_data_new.iloc[indx].shape)\n",
    "print(labels_col[indx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f25d3dd2-cbd8-444f-946d-fc0796b8a8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 15)\n",
      "[[0.06447794 0.06245778 0.12348893 0.10809657 0.11374791 0.10149326\n",
      "  0.08337611 0.07695045 0.14443621 0.12147483]]\n",
      "(1, 10)\n"
     ]
    }
   ],
   "source": [
    "#implementation of the neural network architecture using only numpy.\n",
    "#25 units in hidden layer1, 15 in hidden layer2, 1 last units of for softmax to linear.\n",
    "def sigmoid(z):\n",
    "    tmp = 1/(1+np.exp(-z))\n",
    "    return tmp\n",
    "    \n",
    "def Dense(a_in, W, b):\n",
    "    units = W.shape[1]\n",
    "    a_out = np.zeros((1,units))\n",
    "    for j in range(units):\n",
    "        w = W[:,j]\n",
    "        tmp = np.dot(w,a_in)+b[:,j]\n",
    "        a_out[0][j] = tmp\n",
    "    return a_out\n",
    "\n",
    "def Relu_layer(a_in,W,b):\n",
    "    a_out = np.zeros((1,W.shape[1]))\n",
    "    zero_Arr = np.zeros((1,W.shape[1]))\n",
    "    #a_out = np.maximum((np.dot(W.T, a_in)).T + b, zero_Arr)  # Apply ReLU activation using vectorized operation\n",
    "    a_out = np.maximum(np.matmul(a_in,W) + b , zero_Arr)\n",
    "    return a_out #.T in the end is for taking the transform.\n",
    "\n",
    "#def Softmax_layer(a_in, W,b):\n",
    "    #m,n = W.shape #in this case; m = 15,n = 10\n",
    "    #a_out = np.zeros((1,n))\n",
    "    #sum_deno = 0\n",
    "    #for i in range(n):\n",
    "        #z = np.dot(a_in.T,W[:,i].reshape((m,1)))+b[:,i]\n",
    "        #sum_deno += np.exp(z)\n",
    "    #for k in range(n):\n",
    "        #a_out[:,k] = (np.exp(np.dot(a_in.T,W[:,k].reshape((m,1)))+b[:,k]))/sum_deno\n",
    "    #return a_out\n",
    "def value_ain(x,W1,W2,b1,b2):\n",
    "    a1 = Relu_layer(x,W1,b1)\n",
    "    a2 = Relu_layer(a1,W2,b2)\n",
    "    return a2\n",
    "    \n",
    "#def Softmax_layer(a_in, W, b):\n",
    "    #m, n = W.shape  # Shape of weights: (input_size, output_size)\n",
    "    #z = np.dot(a_in.T, W) + b  # Logits calculation without transposition\n",
    "    #z -= np.max(z, axis=1, keepdims=True)  # Logit scaling to prevent overflow\n",
    "    #exp_z = np.exp(z)\n",
    "    #sum_exp_z = np.sum(exp_z, axis=1, keepdims=True)\n",
    "    #a_out = exp_z / sum_exp_z  # Softmax activation\n",
    "    #return a_out\n",
    "def find_max_ain(a_in):\n",
    "    max_val = a_in[0, 0]  # Initialize max_val with the first element\n",
    "    for i in range(1, a_in.shape[0]):  # Loop through the remaining elements\n",
    "        if a_in[i, 0] > max_val:\n",
    "            max_val = a_in[i, 0]  # Update max_val if a larger element is found\n",
    "    return max_val\n",
    "    \n",
    "def Softmax_layer(a_in, W, b): #should  I incorporate the log(C) = -max(a_in)\n",
    "    logits = np.zeros((1,W.shape[1]))\n",
    "    for k in range(W.shape[1]):\n",
    "        w = W[:,k].reshape((15,1))\n",
    "        logits[:,k] = np.dot(a_in,w) + b[:,k]\n",
    "\n",
    "    logc = np.max(logits)\n",
    "    logits -= logc\n",
    "    a_out = np.zeros((1, W.shape[1]))\n",
    "    \n",
    "    sum_exp = 0\n",
    "    for i in range(W.shape[1]):\n",
    "        sum_exp += np.exp(logits[:,i])\n",
    "    for j in range(W.shape[1]):\n",
    "        a_out[:,j] = (np.exp(logits[:,j]))/sum_exp\n",
    "\n",
    "    return a_out\n",
    "    \n",
    "\n",
    "in_val = train_data_new.iloc[122].values.reshape((1,784))/255.0\n",
    "a2 = value_ain(in_val, W1, W2, b1,b2)\n",
    "print(a2.shape)\n",
    "\n",
    "out_value = Softmax_layer(a2, W3, b3)\n",
    "print(out_value)\n",
    "#so now we see that both the relu and the softmax seem to be working.\n",
    "print(out_value.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "994b7726-1412-4478-a3b4-6d01e6b579c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 25)\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]]\n",
      "(1, 10)\n"
     ]
    }
   ],
   "source": [
    "#the mnist database is for the numbers from 0-9, so for the softmax layer we will have to make 10 different o/ps\n",
    "def Sequential(X,W1,W2,W3,b1,b2,b3):\n",
    "    a1 = Relu_layer(X,W1,b1)\n",
    "    a2 = Relu_layer(a1,W2,b2)\n",
    "    a3 = Softmax_layer(a2,W3,b3)\n",
    "    return a3\n",
    "\n",
    "relu_test = Relu_layer(in_val,W1,b1)\n",
    "print(relu_test.shape)\n",
    "print(relu_test)\n",
    "\n",
    "temp_testcase = train_data_new.iloc[12].values.reshape((1,784))/255.0\n",
    "temp_val = Sequential(temp_testcase, W1, W2, W3, b1, b2, b3)\n",
    "print(temp_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ed198019-f673-4654-93e4-d27ecbf602a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 25)\n",
      "(15, 10)\n",
      "[[5.97208743e-86 1.77296203e-24 3.74269982e-45 6.30100470e-42\n",
      "  1.79899056e-58 2.67840566e-34 1.00000000e+00 1.07530085e-21\n",
      "  2.01214967e-23 2.01161732e-30]]\n"
     ]
    }
   ],
   "source": [
    "def he_initialization(n_prev_layer, n_curr_layer):\n",
    "    return np.random.randn(n_prev_layer,n_curr_layer)*(np.sqrt(2/n_prev_layer))\n",
    "\n",
    "W1 = he_initialization(784,25)\n",
    "W2 = he_initialization(25,15)\n",
    "W3 = he_initialization(15,10)\n",
    "\n",
    "b1 = np.zeros((1,25))\n",
    "b2 = np.zeros((1,15))\n",
    "b3 = np.zeros((1,10))\n",
    "\n",
    "print(W1.shape)\n",
    "print(W3.shape)\n",
    "\n",
    "\n",
    "a2 = np.zeros((1,15))\n",
    "a2 = value_ain(train_data_new.iloc[6].values.reshape((1,784)), W1, W2, b1, b2)\n",
    "a_in = Softmax_layer(a2, W3, b3)\n",
    "print(a_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9aa2f3ed-d1d8-421d-9d68-3eef7aad075d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 15)\n",
      "(1, 25)\n",
      "[[  0.          98.82667723   0.           0.           0.\n",
      "    0.           0.         268.71971159   0.          16.19535866\n",
      "    0.           0.           0.          71.00286936   0.\n",
      "   17.46782378   0.          59.53599677 127.16966326   0.\n",
      "   79.09731416   0.           0.           0.          47.99541886]]\n"
     ]
    }
   ],
   "source": [
    "#gradient descent function to train the softmax layer:\n",
    "#first we define the loss function \n",
    "\n",
    "#def Loss_softmax(a_out, y):\n",
    "    #epsilon = 1e-7\n",
    "    #loss = -np.log(a_out[:,y] + epsilon)  #maybe should be y+1?\n",
    "    #return loss\n",
    "\n",
    "#def threshold(a_in):\n",
    "    #if(a_in>=0.5):\n",
    "    #return 1\n",
    "    #else:\n",
    "        #return 0\n",
    "\n",
    "\n",
    "def layer1_op(one_testcase, W1, b1):\n",
    "    a1 = Relu_layer(one_testcase, W1, b1)\n",
    "    return a1\n",
    "\n",
    "x = train_data_new.iloc[5].values.reshape((1,784))\n",
    "m = value_ain(x,W1,W2,b1,b2)\n",
    "print(m.shape)\n",
    "\n",
    "y = train_data_new.iloc[10].values.reshape((1,784))\n",
    "print(layer1_op(y, W1, b1).shape)\n",
    "print(layer1_op(y, W1, b1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "9a90236d-b06c-4d51-80ee-13cb6145a4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 10)\n",
      "[[ 0.07814119 -0.90548804  0.10332898  0.10226572  0.08723517  0.12708319\n",
      "   0.10410751  0.09599446  0.10064177  0.10669005]]\n",
      "(1, 10)\n",
      "[[0.         0.115445   0.         0.         0.24161349 0.10186623\n",
      "  0.08524261 0.         0.         0.23326737 0.         0.\n",
      "  0.22240874 0.         0.         0.02192371 0.         0.\n",
      "  0.5168824  0.21153329 0.         0.         0.36230087 0.\n",
      "  0.20942899]]\n",
      "(1, 25)\n"
     ]
    }
   ],
   "source": [
    "#now it is important to note that we will require the dJ_dw3, dJ_dw2, dJ_dw1 as well, this is to do backpropagation.\n",
    "def cal_grad_l3(one_testcase,W1_, W2_, b1_, b2_, W3_, b3_, y_):\n",
    "    #y = label_col[indx]    #one testcase is the 784 pixel values of one training example.\n",
    "    m,n = W3_.shape #in this case, m has a value if 15 and n has a value of 10\n",
    "    dJ_dw3 = np.zeros((m,n))\n",
    "    dJ_db3 = np.zeros((1,n))\n",
    "    #this array will eventually carry the w values of all the w in the last layer.\n",
    "    activations_prev = value_ain(one_testcase, W1_, W2_,b1_, b2_)\n",
    "    a_op = Softmax_layer(activations_prev, W3_, b3_) ######there is an issue here.\n",
    "    for i in range(n):\n",
    "        if(i == y_): #y must be a single value and not an array\n",
    "            y_in = 1 \n",
    "        else:\n",
    "            y_in = 0\n",
    "        for j in range(m):\n",
    "            dJ_dw3[j,i] = np.dot(a_op[:, i] - y_in, activations_prev[:, j])#(a_op[:,i] - y_in)*(activations_prev[:,j])\n",
    "        dJ_db3[:,i] = (a_op[:,i] - y_in)\n",
    "\n",
    "    return dJ_dw3, dJ_db3\n",
    "\n",
    "def layer1_op(one_testcase, W1_, b1_):\n",
    "    a1 = Relu_layer(one_testcase, W1_, b1_)\n",
    "    return a1\n",
    "\n",
    "one_test = train_data_new.iloc[14].values.reshape((1,784))/255.0\n",
    "y_val = labels_col[14]\n",
    "dj_dw3, dj_db3 = cal_grad_l3(one_test, W1, W2, b1, b2, W3, b3, y_val)\n",
    "print(dj_dw3.shape)\n",
    "print(dj_db3)\n",
    "print(dj_db3.shape)\n",
    "\n",
    "output = layer1_op(one_test, W1,b1)\n",
    "print(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "835da43b-420c-4b09-bcf0-a4aadb9cef5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...  0. -0.  0.]\n",
      " [ 0.  0.  0. ...  0. -0.  0.]\n",
      " [ 0.  0.  0. ...  0. -0.  0.]\n",
      " ...\n",
      " [ 0.  0.  0. ...  0. -0.  0.]\n",
      " [ 0.  0.  0. ...  0. -0.  0.]\n",
      " [ 0.  0.  0. ...  0. -0.  0.]]\n",
      "---------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#now the other calulate gradient functions are for the backpropagation through the previous layers.\n",
    "def cal_grad_l2l1(one_testcase,_W1,_W2,_W3,_b1,_b2, _b3, y):\n",
    "    g, h = _W1.shape  #one_testcase has to have dimensions 784, 1 for this to function.\n",
    "    m, n = _W2.shape #in this case, m has a value of 25 and n has a value of 15\n",
    "    dJ_dw2 = np.zeros((m,n)) \n",
    "    dJ_db2 = np.zeros((1,n))\n",
    "    dJ_dw1 = np.zeros((g,h))\n",
    "    dJ_db1 = np.zeros((1,h))\n",
    "    del1 = np.zeros((1,h)) #was initially de1 = np.zeros((2,25))\n",
    "    z2 = layer1_op(one_testcase,_W1,_b1).reshape((1,h))\n",
    "    activations_prev = value_ain(one_testcase, _W1, _W2,_b1, _b2).reshape((1,n))\n",
    "    output_values = Softmax_layer(activations_prev, _W3, _b3)\n",
    "    del3 = np.zeros((1,10))\n",
    "    label_val = np.zeros((1,10))\n",
    "    layer2_ip = layer1_op(one_testcase, _W1, _b1).reshape((1,m))\n",
    "    for k in range(10):\n",
    "        if(k == y):\n",
    "            label_val[:,k] = 1\n",
    "        else:\n",
    "            label_val[:,k] = 0\n",
    "    for i in range(10):\n",
    "        del3[:,i] = output_values[:,i] - label_val[:,i] #del3 will be having shape (1,10)\n",
    "    del2 = np.zeros((1,n))\n",
    "    relu_deri = np.zeros((1,n))\n",
    "    for t in range(n):\n",
    "        if(activations_prev[:,t] > 0):\n",
    "            relu_deri[:,t] = 1\n",
    "        else:\n",
    "            relu_deri[:,t] = 0\n",
    "    del2 = np.matmul(del3, _W3.T)* relu_deri\n",
    "    deri_relu_l1 = np.zeros((1,h))\n",
    "    for i1 in range(n): #n is 15 in this case \n",
    "        for j1 in range(m): #m is 25in this case\n",
    "            dJ_dw2[j1,i1] = np.dot(del2[:,i1] , layer2_ip[:,j1]) \n",
    "        dJ_db2[:,i1] = del2[:,i1]    #this can be an issue.\n",
    "    for b in range(h):\n",
    "        if(z2[:,b] > 0):\n",
    "            deri_relu_l1[:,b] = 1\n",
    "        else:\n",
    "            deri_relu_l1[:,b] = 0\n",
    "    del1 = np.matmul(del2,_W2.T)*deri_relu_l1\n",
    "    for outer in range(h): #this one evaluates to 25  \n",
    "        for inner in range(g): # this evaluates to 784\n",
    "            dJ_dw1[inner,outer] = np.dot(del1[:,outer] ,one_testcase[:,inner]) #the issue persists i\n",
    "        dJ_db1[:,outer] = del1[:,outer]\n",
    "        \n",
    "    return dJ_dw2, dJ_dw1 , dJ_db2 , dJ_db1\n",
    "new_W1 = W1\n",
    "new_W2 = W2\n",
    "new_W3 = W3\n",
    "new_b1 = b1\n",
    "new_b2 = b2\n",
    "new_b3 = b3\n",
    "derJ_dw2, derj_dw1, derJ_db2, derJ_db1 = cal_grad_l2l1(one_test,new_W1,new_W2,new_W3,new_b1,new_b2,new_b3,y_val)\n",
    "print(derj_dw1)\n",
    "#temp_W1 = new_W1 - 0.1*derj_dw1\n",
    "#print(new_W1)\n",
    "print(\"---------------------------------------------------------------------\")\n",
    "#print(temp_W1)\n",
    "#the calculation of gradients for W1 is not taking place. This needs to be fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "a8f703ba-47e5-417e-b9ae-17ec3930f542",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we now need a loss function and then gradient descent algorithm.\n",
    "def softmax_loss(one_testcase, W_1, W_2, W_3, b_1, b_2, b_3, y):\n",
    "    output = Sequential(one_testcase , W_1, W_2, W_3, b_1, b_2, b_3)\n",
    "    loss = -np.log(output[0,int(y)])\n",
    "    return loss.item()\n",
    "            \n",
    "            \n",
    "#gradient descent will be run on batches of 10 examples \n",
    "def gradient_descent_algo(batch_100 , W_3, W_2, W_1, b_3, b_2, b_1, corresp_labels, learning_rate):\n",
    "    alpha = learning_rate\n",
    "    #L_batch_hist = []\n",
    "    #W3_batch_hist = []\n",
    "    #W2_batch_hist = []\n",
    "    #W1_batch_hist = []\n",
    "    \n",
    "    #num_iters = batch_100.shape[0]\n",
    "    #for k in range(num_iters):\n",
    "        #dJ_dw3, dJ_db3 = cal_grad_l3(batch_100[k,:].reshape((1,784))/255.0,W_1, W_2, b_1, b_2, W_3, b_3, corresp_labels[k,:]) #need to figure out what the y is here\n",
    "        #W_3 = W_3 - (alpha)*dJ_dw3\n",
    "        #b_3 = b_3 - (alpha)*dJ_db3\n",
    "\n",
    "        #dJ_dw2 , dJ_dw1 , dJ_db2, dJ_db1 = cal_grad_l2l1(batch_100[k,:].reshape((1,784))/255.0, W_1, W_2,W_3,b_1, b_2, b_3 , corresp_labels[k,:])\n",
    "        #W_2 = W_2 - (alpha)*dJ_dw2 \n",
    "        #b_2 = b_2 - (alpha)*dJ_db2\n",
    "        #W_1 = W_1 - (alpha)*dJ_dw1 \n",
    "        #b_1 = b_1 - (alpha)*dJ_db1\n",
    "\n",
    "        #loss = softmax_loss(batch_100[k,:], W_1, W_2, W_3, b_1, b_2, b_3, corresp_labels[k,:]) #the error is due to the fact that this is an array.\n",
    "        #L_batch_hist.append(loss)\n",
    "\n",
    "        #if(k % 5 == 0):\n",
    "            #W3_batch_hist.append(W_3.copy())\n",
    "            #W2_batch_hist.append(W_2.copy())\n",
    "            #W1_batch_hist.append(W_1.copy())\n",
    "\n",
    "    #print(f\"The value of W_1 is {W_1}\")\n",
    "    #print(f\"Loss: {loss:.4f}\")\n",
    "\n",
    "    #return W_3, W_2, W_1, b_3, b_2, b_1\n",
    "\n",
    "def gradient_descent_algo(batch_100 , W_3, W_2, W_1, b_3, b_2, b_1, corresp_labels, learning_rate):\n",
    "    alpha = learning_rate\n",
    "    \n",
    "    num_iters = batch_100.shape[0]\n",
    "    for k in range(num_iters):\n",
    "        # Compute gradients\n",
    "        dJ_dw3, dJ_db3 = cal_grad_l3(batch_100[k, :].reshape((1, 784)) / 255.0, W_1, W_2, b_1, b_2, W_3, b_3, corresp_labels[k, :])\n",
    "        dJ_dw2, dJ_dw1, dJ_db2, dJ_db1 = cal_grad_l2l1(batch_100[k, :].reshape((1, 784)) / 255.0, W_1, W_2, W_3, b_1, b_2, b_3, corresp_labels[k, :])\n",
    "\n",
    "        # Update weights and biases\n",
    "        W_3 -= alpha * dJ_dw3\n",
    "        b_3 -= alpha * dJ_db3\n",
    "\n",
    "        W_2 -= alpha * dJ_dw2\n",
    "        b_2 -= alpha * dJ_db2\n",
    "\n",
    "        W_1 -= alpha * dJ_dw1\n",
    "        b_1 -= alpha * dJ_db1\n",
    "\n",
    "        # Compute and print loss\n",
    "        loss = softmax_loss(batch_100[k, :], W_1, W_2, W_3, b_1, b_2, b_3, corresp_labels[k, :])\n",
    "        if k % 10 == 0:  # Print every 10 iterations\n",
    "            print(f\"Iteration {k}, Loss: {loss:.4f}\")\n",
    "            print(f\"W_2: {W_2}\")\n",
    "\n",
    "    return W_3, W_2, W_1, b_3, b_2, b_1\n",
    "        \n",
    "#test_W3, test_W2, test_W1, test_b3,test_b2,test_b1 = gradient_descent_algo(train_data_new[:100], new_W3, new_W2,new_W1, new_b3, new_b2, new_b1, labels_col[:100],0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "14f184b9-f23a-4edc-a61d-81c73ffef4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#def grad_desc_datafeeder(training_data_new, labels_col_arr,W_global3,W_global2, W_global1, b_global3, b_global2, b_global1,learning_rate): #shape of train_data_new is (10000,784) and the shape of labels_col is (10000,1)\n",
    "    #rows_per_run = 10\n",
    "    #total_rows = train_data_new.shape[0]\n",
    "    #num_epoch = total_rows//rows_per_run\n",
    "    #for i in range(num_epoch): #this range is 1000\n",
    "        #push_arr = np.zeros((10,784))\n",
    "        #push_labels = np.zeros((10,1))\n",
    "        #for j in range(rows_per_run):#this range is 10.\n",
    "            #temp_arr = np.zeros((1,784))\n",
    "            #temp_arr = training_data_new.iloc[10*i + j].values.reshape((1,784))\n",
    "            #push_labels[j,:] = labels_col_arr.iloc[10*i+j]\n",
    "            #push_arr[j,:] = temp_arr\n",
    "        #W_global3,W_global2, W_global1, b_global3, b_global2, b_global1 = gradient_descent_algo(push_arr, W_global3,W_global2, W_global1, b_global3, b_global2, b_global1,push_labels,learning_rate)\n",
    "        #return W_global3,W_global2, W_global1, b_global3, b_global2, b_global1\n",
    "\n",
    "def grad_desc_datafeeder(training_data_new, labels_col_arr, W_global3, W_global2, W_global1, b_global3, b_global2, b_global1, learning_rate):\n",
    "    rows_per_run = 100\n",
    "    total_rows = training_data_new.shape[0]\n",
    "    num_epoch = total_rows // rows_per_run\n",
    "    \n",
    "    for i in range(num_epoch):\n",
    "        push_arr = np.zeros((rows_per_run, 784))\n",
    "        push_labels = np.zeros((rows_per_run, 1))\n",
    "        \n",
    "        for j in range(rows_per_run):\n",
    "            index = 100 * i + j\n",
    "            push_arr[j, :] = training_data_new.iloc[index].values.reshape((1,784))\n",
    "            push_labels[j, :] = labels_col_arr.iloc[index]\n",
    "            \n",
    "        W_global3, W_global2, W_global1, b_global3, b_global2, b_global1 = gradient_descent_algo(\n",
    "            push_arr, W_global3, W_global2, W_global1, b_global3, b_global2, b_global1, push_labels, learning_rate\n",
    "        )\n",
    "    \n",
    "    return W_global3, W_global2, W_global1, b_global3, b_global2, b_global1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "bcfc83fe-1ee8-4dda-84ec-5eb7b88b4d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12171/1838538200.py:4: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  loss = -np.log(output[0,int(y)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: -0.0000\n",
      "Iteration 10, Loss: -0.0000\n",
      "Iteration 20, Loss: 0.0000\n",
      "Iteration 30, Loss: 58.1628\n",
      "Iteration 40, Loss: -0.0000\n",
      "Iteration 50, Loss: 53.7491\n",
      "Iteration 60, Loss: -0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12171/1838538200.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -np.log(output[0,int(y)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 70, Loss: -0.0000\n",
      "Iteration 80, Loss: -0.0000\n",
      "Iteration 90, Loss: 0.1734\n",
      "Iteration 0, Loss: 6.1504\n",
      "Iteration 10, Loss: 21.4832\n",
      "Iteration 20, Loss: 8.8384\n",
      "Iteration 30, Loss: 24.4903\n",
      "Iteration 40, Loss: 53.4976\n",
      "Iteration 50, Loss: 0.0377\n",
      "Iteration 60, Loss: 113.4842\n",
      "Iteration 70, Loss: -0.0000\n",
      "Iteration 80, Loss: -0.0000\n",
      "Iteration 90, Loss: -0.0000\n",
      "Iteration 0, Loss: 86.1751\n",
      "Iteration 10, Loss: -0.0000\n",
      "Iteration 20, Loss: -0.0000\n",
      "Iteration 30, Loss: 11.9092\n",
      "Iteration 40, Loss: 0.8991\n",
      "Iteration 50, Loss: -0.0000\n",
      "Iteration 60, Loss: -0.0000\n",
      "Iteration 70, Loss: 170.2783\n",
      "Iteration 80, Loss: 50.0807\n",
      "Iteration 90, Loss: 9.1474\n",
      "Iteration 0, Loss: 7.3009\n",
      "Iteration 10, Loss: 47.6047\n",
      "Iteration 20, Loss: 8.6810\n",
      "Iteration 30, Loss: -0.0000\n",
      "Iteration 40, Loss: -0.0000\n",
      "Iteration 50, Loss: -0.0000\n",
      "Iteration 60, Loss: -0.0000\n",
      "Iteration 70, Loss: -0.0000\n",
      "Iteration 80, Loss: -0.0000\n",
      "Iteration 90, Loss: 1.6610\n",
      "Iteration 0, Loss: 1.5733\n",
      "Iteration 10, Loss: -0.0000\n",
      "Iteration 20, Loss: 2.5746\n",
      "Iteration 30, Loss: -0.0000\n",
      "Iteration 40, Loss: -0.0000\n",
      "Iteration 50, Loss: -0.0000\n",
      "Iteration 60, Loss: 15.8432\n",
      "Iteration 70, Loss: 0.0000\n",
      "Iteration 80, Loss: -0.0000\n",
      "Iteration 90, Loss: -0.0000\n",
      "Iteration 0, Loss: 37.6987\n",
      "Iteration 10, Loss: 2.5337\n",
      "Iteration 20, Loss: 14.3476\n",
      "Iteration 30, Loss: 1.9750\n",
      "Iteration 40, Loss: -0.0000\n",
      "Iteration 50, Loss: -0.0000\n",
      "Iteration 60, Loss: -0.0000\n",
      "Iteration 70, Loss: 2.3070\n",
      "Iteration 80, Loss: 1.8232\n",
      "Iteration 90, Loss: -0.0000\n",
      "Iteration 0, Loss: 82.9074\n",
      "Iteration 10, Loss: 2.1997\n",
      "Iteration 20, Loss: -0.0000\n",
      "Iteration 30, Loss: 2.1902\n",
      "Iteration 40, Loss: -0.0000\n",
      "Iteration 50, Loss: 0.0000\n",
      "Iteration 60, Loss: -0.0000\n",
      "Iteration 70, Loss: 1.9209\n",
      "Iteration 80, Loss: 2.3131\n",
      "Iteration 90, Loss: 1.7886\n",
      "Iteration 0, Loss: -0.0000\n",
      "Iteration 10, Loss: 341.3618\n",
      "Iteration 20, Loss: 80.4628\n",
      "Iteration 30, Loss: -0.0000\n",
      "Iteration 40, Loss: -0.0000\n",
      "Iteration 50, Loss: -0.0000\n",
      "Iteration 60, Loss: 2.3989\n",
      "Iteration 70, Loss: -0.0000\n",
      "Iteration 80, Loss: -0.0000\n",
      "Iteration 90, Loss: -0.0000\n",
      "Iteration 0, Loss: 10.6734\n",
      "Iteration 10, Loss: -0.0000\n",
      "Iteration 20, Loss: 0.0000\n",
      "Iteration 30, Loss: 1.3765\n",
      "Iteration 40, Loss: -0.0000\n",
      "Iteration 50, Loss: -0.0000\n",
      "Iteration 60, Loss: 1.9594\n",
      "Iteration 70, Loss: 0.0000\n",
      "Iteration 80, Loss: -0.0000\n",
      "Iteration 90, Loss: -0.0000\n",
      "Iteration 0, Loss: -0.0000\n",
      "Iteration 10, Loss: -0.0000\n",
      "Iteration 20, Loss: -0.0000\n",
      "Iteration 30, Loss: 510.0881\n",
      "Iteration 40, Loss: 1.9685\n",
      "Iteration 50, Loss: 2.8261\n",
      "Iteration 60, Loss: -0.0000\n",
      "Iteration 70, Loss: 1.9946\n",
      "Iteration 80, Loss: 1.4020\n",
      "Iteration 90, Loss: 1.3618\n",
      "Iteration 0, Loss: -0.0000\n",
      "Iteration 10, Loss: 515.0692\n",
      "Iteration 20, Loss: 177.6460\n",
      "Iteration 30, Loss: -0.0000\n",
      "Iteration 40, Loss: -0.0000\n",
      "Iteration 50, Loss: 1.4386\n",
      "Iteration 60, Loss: -0.0000\n",
      "Iteration 70, Loss: inf\n",
      "Iteration 80, Loss: -0.0000\n",
      "Iteration 90, Loss: 89.7083\n",
      "Iteration 0, Loss: -0.0000\n",
      "Iteration 10, Loss: -0.0000\n",
      "Iteration 20, Loss: -0.0000\n",
      "Iteration 30, Loss: 28.0552\n",
      "Iteration 40, Loss: -0.0000\n",
      "Iteration 50, Loss: 1.6377\n",
      "Iteration 60, Loss: -0.0000\n",
      "Iteration 70, Loss: 1.7640\n",
      "Iteration 80, Loss: -0.0000\n",
      "Iteration 90, Loss: 1.9364\n",
      "Iteration 0, Loss: 21.3020\n",
      "Iteration 10, Loss: 2.0732\n",
      "Iteration 20, Loss: -0.0000\n",
      "Iteration 30, Loss: -0.0000\n",
      "Iteration 40, Loss: -0.0000\n",
      "Iteration 50, Loss: 2.1335\n",
      "Iteration 60, Loss: 2.7014\n",
      "Iteration 70, Loss: -0.0000\n",
      "Iteration 80, Loss: 80.8941\n",
      "Iteration 90, Loss: -0.0000\n",
      "Iteration 0, Loss: -0.0000\n",
      "Iteration 10, Loss: -0.0000\n",
      "Iteration 20, Loss: -0.0000\n",
      "Iteration 30, Loss: -0.0000\n",
      "Iteration 40, Loss: 16.9217\n",
      "Iteration 50, Loss: -0.0000\n",
      "Iteration 60, Loss: -0.0000\n",
      "Iteration 70, Loss: 2.3141\n",
      "Iteration 80, Loss: 0.0000\n",
      "Iteration 90, Loss: -0.0000\n",
      "Iteration 0, Loss: 1.4308\n",
      "Iteration 10, Loss: 0.9732\n",
      "Iteration 20, Loss: 150.4473\n",
      "Iteration 30, Loss: -0.0000\n",
      "Iteration 40, Loss: 47.3975\n",
      "Iteration 50, Loss: 57.3129\n",
      "Iteration 60, Loss: 1.9602\n",
      "Iteration 70, Loss: -0.0000\n",
      "Iteration 80, Loss: 1.9263\n",
      "Iteration 90, Loss: 1.8018\n",
      "Iteration 0, Loss: 478.7718\n",
      "Iteration 10, Loss: 1.8643\n",
      "Iteration 20, Loss: 3.1271\n",
      "Iteration 30, Loss: 2.4270\n",
      "Iteration 40, Loss: -0.0000\n",
      "Iteration 50, Loss: 2.0121\n",
      "Iteration 60, Loss: 1.6125\n",
      "Iteration 70, Loss: -0.0000\n",
      "Iteration 80, Loss: -0.0000\n",
      "Iteration 90, Loss: -0.0000\n",
      "Iteration 0, Loss: 1.6627\n",
      "Iteration 10, Loss: -0.0000\n",
      "Iteration 20, Loss: 1.3348\n",
      "Iteration 30, Loss: -0.0000\n",
      "Iteration 40, Loss: 293.1113\n",
      "Iteration 50, Loss: -0.0000\n",
      "Iteration 60, Loss: -0.0000\n",
      "Iteration 70, Loss: 1.9887\n",
      "Iteration 80, Loss: -0.0000\n",
      "Iteration 90, Loss: 1.3751\n",
      "Iteration 0, Loss: 2.6658\n",
      "Iteration 10, Loss: -0.0000\n",
      "Iteration 20, Loss: 7.2240\n",
      "Iteration 30, Loss: 2.7507\n",
      "Iteration 40, Loss: 3.2057\n",
      "Iteration 50, Loss: 1.0614\n",
      "Iteration 60, Loss: -0.0000\n",
      "Iteration 70, Loss: -0.0000\n",
      "Iteration 80, Loss: -0.0000\n",
      "Iteration 90, Loss: 7.1223\n",
      "Iteration 0, Loss: 7.1815\n",
      "Iteration 10, Loss: inf\n",
      "Iteration 20, Loss: -0.0000\n",
      "Iteration 30, Loss: -0.0000\n",
      "Iteration 40, Loss: 1.3945\n",
      "Iteration 50, Loss: 2.2195\n",
      "Iteration 60, Loss: 2.1317\n",
      "Iteration 70, Loss: -0.0000\n",
      "Iteration 80, Loss: 2.4625\n",
      "Iteration 90, Loss: -0.0000\n",
      "Iteration 0, Loss: -0.0000\n",
      "Iteration 10, Loss: 1.9548\n",
      "Iteration 20, Loss: 16.9034\n",
      "Iteration 30, Loss: 3.3861\n",
      "Iteration 40, Loss: 1.8420\n",
      "Iteration 50, Loss: -0.0000\n",
      "Iteration 60, Loss: 322.1274\n",
      "Iteration 70, Loss: 1.7642\n",
      "Iteration 80, Loss: 4.5537\n",
      "Iteration 90, Loss: 2.3043\n",
      "Iteration 0, Loss: 0.3488\n",
      "Iteration 10, Loss: -0.0000\n",
      "Iteration 20, Loss: 1.8080\n",
      "Iteration 30, Loss: 1.7776\n",
      "Iteration 40, Loss: 2.4189\n",
      "Iteration 50, Loss: 2.4164\n",
      "Iteration 60, Loss: 2.5244\n",
      "Iteration 70, Loss: 2.4196\n",
      "Iteration 80, Loss: 1.7412\n",
      "Iteration 90, Loss: 1.9825\n",
      "Iteration 0, Loss: 1.9284\n",
      "Iteration 10, Loss: 2.4318\n",
      "Iteration 20, Loss: 2.0389\n",
      "Iteration 30, Loss: 1.9636\n",
      "Iteration 40, Loss: 2.2775\n",
      "Iteration 50, Loss: 2.1924\n",
      "Iteration 60, Loss: 2.0992\n",
      "Iteration 70, Loss: 2.1043\n",
      "Iteration 80, Loss: 2.1026\n",
      "Iteration 90, Loss: 2.3130\n",
      "Iteration 0, Loss: 2.0027\n",
      "Iteration 10, Loss: 2.2238\n",
      "Iteration 20, Loss: 2.5273\n",
      "Iteration 30, Loss: 2.3007\n",
      "Iteration 40, Loss: 2.3519\n",
      "Iteration 50, Loss: 2.2619\n",
      "Iteration 60, Loss: 2.1192\n",
      "Iteration 70, Loss: 2.1473\n",
      "Iteration 80, Loss: 2.5053\n",
      "Iteration 90, Loss: 1.9849\n",
      "Iteration 0, Loss: 2.2901\n",
      "Iteration 10, Loss: 2.2121\n",
      "Iteration 20, Loss: 2.1264\n",
      "Iteration 30, Loss: 2.2110\n",
      "Iteration 40, Loss: 2.0236\n",
      "Iteration 50, Loss: 2.4121\n",
      "Iteration 60, Loss: 2.5289\n",
      "Iteration 70, Loss: 2.4658\n",
      "Iteration 80, Loss: 2.2243\n",
      "Iteration 90, Loss: 2.3830\n",
      "Iteration 0, Loss: 2.6362\n",
      "Iteration 10, Loss: 2.2579\n",
      "Iteration 20, Loss: 2.2302\n",
      "Iteration 30, Loss: 2.1478\n",
      "Iteration 40, Loss: 2.1000\n",
      "Iteration 50, Loss: 2.2322\n",
      "Iteration 60, Loss: 2.6356\n",
      "Iteration 70, Loss: 2.2447\n",
      "Iteration 80, Loss: 2.4309\n",
      "Iteration 90, Loss: 2.2297\n",
      "Iteration 0, Loss: 1.7748\n",
      "Iteration 10, Loss: 2.3884\n",
      "Iteration 20, Loss: 2.4802\n",
      "Iteration 30, Loss: 2.1889\n",
      "Iteration 40, Loss: 2.4840\n",
      "Iteration 50, Loss: 2.4966\n",
      "Iteration 60, Loss: 2.1354\n",
      "Iteration 70, Loss: 2.2209\n",
      "Iteration 80, Loss: 2.4287\n",
      "Iteration 90, Loss: 2.4309\n",
      "Iteration 0, Loss: 2.3442\n",
      "Iteration 10, Loss: 2.4023\n",
      "Iteration 20, Loss: 2.4555\n",
      "Iteration 30, Loss: 2.6190\n",
      "Iteration 40, Loss: 2.2360\n",
      "Iteration 50, Loss: 2.2391\n",
      "Iteration 60, Loss: 2.0292\n",
      "Iteration 70, Loss: 2.1009\n",
      "Iteration 80, Loss: 2.3674\n",
      "Iteration 90, Loss: 2.0327\n",
      "Iteration 0, Loss: 2.1225\n",
      "Iteration 10, Loss: 2.1315\n",
      "Iteration 20, Loss: 2.3570\n",
      "Iteration 30, Loss: 2.2426\n",
      "Iteration 40, Loss: 2.3322\n",
      "Iteration 50, Loss: 2.1593\n",
      "Iteration 60, Loss: 2.2378\n",
      "Iteration 70, Loss: 2.1493\n",
      "Iteration 80, Loss: 2.5259\n",
      "Iteration 90, Loss: 2.2201\n",
      "Iteration 0, Loss: 2.4617\n",
      "Iteration 10, Loss: 2.0473\n",
      "Iteration 20, Loss: 2.6160\n",
      "Iteration 30, Loss: 2.0438\n",
      "Iteration 40, Loss: 2.3049\n",
      "Iteration 50, Loss: 2.0421\n",
      "Iteration 60, Loss: 2.1004\n",
      "Iteration 70, Loss: 2.2349\n",
      "Iteration 80, Loss: 2.1231\n",
      "Iteration 90, Loss: 2.1631\n",
      "Iteration 0, Loss: 2.0158\n",
      "Iteration 10, Loss: 2.5182\n",
      "Iteration 20, Loss: 2.4268\n",
      "Iteration 30, Loss: 2.0845\n",
      "Iteration 40, Loss: 2.5577\n",
      "Iteration 50, Loss: 2.1760\n",
      "Iteration 60, Loss: 2.3384\n",
      "Iteration 70, Loss: 1.9632\n",
      "Iteration 80, Loss: 2.5111\n",
      "Iteration 90, Loss: 2.4064\n",
      "Iteration 0, Loss: 2.3700\n",
      "Iteration 10, Loss: 2.0317\n",
      "Iteration 20, Loss: 2.1604\n",
      "Iteration 30, Loss: 2.4247\n",
      "Iteration 40, Loss: 2.4888\n",
      "Iteration 50, Loss: 2.0861\n",
      "Iteration 60, Loss: 2.0732\n",
      "Iteration 70, Loss: 2.2038\n",
      "Iteration 80, Loss: 2.1748\n",
      "Iteration 90, Loss: 2.1987\n",
      "Iteration 0, Loss: 2.3936\n",
      "Iteration 10, Loss: 1.8867\n",
      "Iteration 20, Loss: 1.9300\n",
      "Iteration 30, Loss: 2.3379\n",
      "Iteration 40, Loss: 2.5539\n",
      "Iteration 50, Loss: 1.8622\n",
      "Iteration 60, Loss: 2.5592\n",
      "Iteration 70, Loss: 2.6553\n",
      "Iteration 80, Loss: 2.3037\n",
      "Iteration 90, Loss: 2.4973\n",
      "Iteration 0, Loss: 2.4702\n",
      "Iteration 10, Loss: 2.4674\n",
      "Iteration 20, Loss: 2.4082\n",
      "Iteration 30, Loss: 2.1787\n",
      "Iteration 40, Loss: 2.6706\n",
      "Iteration 50, Loss: 2.2151\n",
      "Iteration 60, Loss: 1.9444\n",
      "Iteration 70, Loss: 2.1337\n",
      "Iteration 80, Loss: 1.9319\n",
      "Iteration 90, Loss: 2.3436\n",
      "Iteration 0, Loss: 2.2321\n",
      "Iteration 10, Loss: 2.1579\n",
      "Iteration 20, Loss: 2.0942\n",
      "Iteration 30, Loss: 2.0534\n",
      "Iteration 40, Loss: 2.2302\n",
      "Iteration 50, Loss: 2.1747\n",
      "Iteration 60, Loss: 1.9131\n",
      "Iteration 70, Loss: 2.2946\n",
      "Iteration 80, Loss: 2.2315\n",
      "Iteration 90, Loss: 2.5673\n",
      "Iteration 0, Loss: 1.8892\n",
      "Iteration 10, Loss: 2.1795\n",
      "Iteration 20, Loss: 2.2936\n",
      "Iteration 30, Loss: 2.2135\n",
      "Iteration 40, Loss: 2.5085\n",
      "Iteration 50, Loss: 2.0398\n",
      "Iteration 60, Loss: 2.6355\n",
      "Iteration 70, Loss: 2.5295\n",
      "Iteration 80, Loss: 1.8789\n",
      "Iteration 90, Loss: 2.3090\n",
      "Iteration 0, Loss: 2.1112\n",
      "Iteration 10, Loss: 2.3701\n",
      "Iteration 20, Loss: 9.7334\n",
      "Iteration 30, Loss: 2.1437\n",
      "Iteration 40, Loss: 2.2977\n",
      "Iteration 50, Loss: 2.2518\n",
      "Iteration 60, Loss: 2.0540\n",
      "Iteration 70, Loss: 2.3929\n",
      "Iteration 80, Loss: 2.2682\n",
      "Iteration 90, Loss: 2.4476\n",
      "Iteration 0, Loss: 2.2589\n",
      "Iteration 10, Loss: 1.9956\n",
      "Iteration 20, Loss: 2.5139\n",
      "Iteration 30, Loss: 2.4888\n",
      "Iteration 40, Loss: 2.3323\n",
      "Iteration 50, Loss: 2.2372\n",
      "Iteration 60, Loss: 2.2392\n",
      "Iteration 70, Loss: 2.5045\n",
      "Iteration 80, Loss: 2.2351\n",
      "Iteration 90, Loss: 2.1806\n",
      "Iteration 0, Loss: 2.1881\n",
      "Iteration 10, Loss: 2.1387\n",
      "Iteration 20, Loss: 2.2221\n",
      "Iteration 30, Loss: 2.1405\n",
      "Iteration 40, Loss: 2.3408\n",
      "Iteration 50, Loss: 2.6008\n",
      "Iteration 60, Loss: 2.3793\n",
      "Iteration 70, Loss: 2.2236\n",
      "Iteration 80, Loss: 2.2250\n",
      "Iteration 90, Loss: 2.2758\n",
      "Iteration 0, Loss: 2.4173\n",
      "Iteration 10, Loss: 2.2337\n",
      "Iteration 20, Loss: 2.4831\n",
      "Iteration 30, Loss: 2.1921\n",
      "Iteration 40, Loss: 2.3548\n",
      "Iteration 50, Loss: 2.2537\n",
      "Iteration 60, Loss: 2.2503\n",
      "Iteration 70, Loss: 2.1188\n",
      "Iteration 80, Loss: 2.1503\n",
      "Iteration 90, Loss: 2.3052\n",
      "Iteration 0, Loss: 2.2880\n",
      "Iteration 10, Loss: 2.5801\n",
      "Iteration 20, Loss: 2.6535\n",
      "Iteration 30, Loss: 2.0982\n",
      "Iteration 40, Loss: 2.3607\n",
      "Iteration 50, Loss: 2.0007\n",
      "Iteration 60, Loss: 1.9257\n",
      "Iteration 70, Loss: 1.9576\n",
      "Iteration 80, Loss: 2.4134\n",
      "Iteration 90, Loss: 2.2965\n",
      "Iteration 0, Loss: 1.7762\n",
      "Iteration 10, Loss: 2.1887\n",
      "Iteration 20, Loss: 2.3302\n",
      "Iteration 30, Loss: 1.8535\n",
      "Iteration 40, Loss: 1.9104\n",
      "Iteration 50, Loss: 1.8504\n",
      "Iteration 60, Loss: 2.3319\n",
      "Iteration 70, Loss: 2.2121\n",
      "Iteration 80, Loss: 2.1252\n",
      "Iteration 90, Loss: 2.0464\n",
      "Iteration 0, Loss: 2.5794\n",
      "Iteration 10, Loss: 1.9032\n",
      "Iteration 20, Loss: 2.3370\n",
      "Iteration 30, Loss: 2.2228\n",
      "Iteration 40, Loss: 1.8055\n",
      "Iteration 50, Loss: 2.3877\n",
      "Iteration 60, Loss: 2.5308\n",
      "Iteration 70, Loss: 1.9573\n",
      "Iteration 80, Loss: 2.0157\n",
      "Iteration 90, Loss: 1.7084\n",
      "Iteration 0, Loss: 2.3414\n",
      "Iteration 10, Loss: 2.0354\n",
      "Iteration 20, Loss: 2.7420\n",
      "Iteration 30, Loss: 2.2539\n",
      "Iteration 40, Loss: 1.8997\n",
      "Iteration 50, Loss: 2.0672\n",
      "Iteration 60, Loss: 2.0855\n",
      "Iteration 70, Loss: 2.4894\n",
      "Iteration 80, Loss: 1.9631\n",
      "Iteration 90, Loss: 2.3777\n",
      "Iteration 0, Loss: 2.3868\n",
      "Iteration 10, Loss: 2.1948\n",
      "Iteration 20, Loss: 2.5857\n",
      "Iteration 30, Loss: 2.0492\n",
      "Iteration 40, Loss: 1.9699\n",
      "Iteration 50, Loss: 2.0329\n",
      "Iteration 60, Loss: 1.8297\n",
      "Iteration 70, Loss: 2.1401\n",
      "Iteration 80, Loss: 2.3649\n",
      "Iteration 90, Loss: 2.2538\n",
      "Iteration 0, Loss: 2.7584\n",
      "Iteration 10, Loss: 1.8193\n",
      "Iteration 20, Loss: 2.1090\n",
      "Iteration 30, Loss: 2.4986\n",
      "Iteration 40, Loss: 2.1399\n",
      "Iteration 50, Loss: 2.5200\n",
      "Iteration 60, Loss: 2.6101\n",
      "Iteration 70, Loss: 2.1836\n",
      "Iteration 80, Loss: 1.9686\n",
      "Iteration 90, Loss: 2.1464\n",
      "Iteration 0, Loss: 2.0244\n",
      "Iteration 10, Loss: 1.9889\n",
      "Iteration 20, Loss: 2.5071\n",
      "Iteration 30, Loss: 2.3261\n",
      "Iteration 40, Loss: 2.2181\n",
      "Iteration 50, Loss: 2.1595\n",
      "Iteration 60, Loss: 1.8981\n",
      "Iteration 70, Loss: 1.9346\n",
      "Iteration 80, Loss: 2.3624\n",
      "Iteration 90, Loss: 2.2936\n",
      "Iteration 0, Loss: 2.2861\n",
      "Iteration 10, Loss: 2.0648\n",
      "Iteration 20, Loss: 2.0378\n",
      "Iteration 30, Loss: 2.1991\n",
      "Iteration 40, Loss: 2.1871\n",
      "Iteration 50, Loss: 2.2001\n",
      "Iteration 60, Loss: 2.2486\n",
      "Iteration 70, Loss: 2.0965\n",
      "Iteration 80, Loss: 2.3450\n",
      "Iteration 90, Loss: 2.1880\n",
      "Iteration 0, Loss: 1.7825\n",
      "Iteration 10, Loss: 2.2301\n",
      "Iteration 20, Loss: 2.4557\n",
      "Iteration 30, Loss: 2.2188\n",
      "Iteration 40, Loss: 2.3484\n",
      "Iteration 50, Loss: 2.0762\n",
      "Iteration 60, Loss: 2.0687\n",
      "Iteration 70, Loss: 2.3159\n",
      "Iteration 80, Loss: 2.4221\n",
      "Iteration 90, Loss: 2.3793\n",
      "Iteration 0, Loss: 1.8879\n",
      "Iteration 10, Loss: 2.4796\n",
      "Iteration 20, Loss: 2.0205\n",
      "Iteration 30, Loss: 2.4429\n",
      "Iteration 40, Loss: 2.2487\n",
      "Iteration 50, Loss: 1.7677\n",
      "Iteration 60, Loss: 2.4711\n",
      "Iteration 70, Loss: 2.5488\n",
      "Iteration 80, Loss: 2.4704\n",
      "Iteration 90, Loss: 1.9676\n",
      "Iteration 0, Loss: 2.0723\n",
      "Iteration 10, Loss: 2.2715\n",
      "Iteration 20, Loss: 2.3676\n",
      "Iteration 30, Loss: 1.9862\n",
      "Iteration 40, Loss: 2.2397\n",
      "Iteration 50, Loss: 2.1484\n",
      "Iteration 60, Loss: 2.2424\n",
      "Iteration 70, Loss: 2.1718\n",
      "Iteration 80, Loss: 2.4481\n",
      "Iteration 90, Loss: 2.3954\n",
      "Iteration 0, Loss: 2.2862\n",
      "Iteration 10, Loss: 2.4811\n",
      "Iteration 20, Loss: 2.7529\n",
      "Iteration 30, Loss: 2.4584\n",
      "Iteration 40, Loss: 2.4533\n",
      "Iteration 50, Loss: 1.9187\n",
      "Iteration 60, Loss: 1.9521\n",
      "Iteration 70, Loss: 1.9855\n",
      "Iteration 80, Loss: 1.9303\n",
      "Iteration 90, Loss: 2.1917\n",
      "Iteration 0, Loss: 2.0836\n",
      "Iteration 10, Loss: 2.1999\n",
      "Iteration 20, Loss: 2.0756\n",
      "Iteration 30, Loss: 2.2385\n",
      "Iteration 40, Loss: 2.5738\n",
      "Iteration 50, Loss: 2.3550\n",
      "Iteration 60, Loss: 2.6870\n",
      "Iteration 70, Loss: 2.6511\n",
      "Iteration 80, Loss: 2.0633\n",
      "Iteration 90, Loss: 2.0828\n",
      "Iteration 0, Loss: 2.0607\n",
      "Iteration 10, Loss: 2.4425\n",
      "Iteration 20, Loss: 2.3188\n",
      "Iteration 30, Loss: 2.1908\n",
      "Iteration 40, Loss: 2.2082\n",
      "Iteration 50, Loss: 2.4339\n",
      "Iteration 60, Loss: 2.2417\n",
      "Iteration 70, Loss: 2.0759\n",
      "Iteration 80, Loss: 2.0936\n",
      "Iteration 90, Loss: 2.1096\n",
      "Iteration 0, Loss: 2.2876\n",
      "Iteration 10, Loss: 2.3054\n",
      "Iteration 20, Loss: 2.0727\n",
      "Iteration 30, Loss: 2.2170\n",
      "Iteration 40, Loss: 2.3793\n",
      "Iteration 50, Loss: 2.1328\n",
      "Iteration 60, Loss: 2.4257\n",
      "Iteration 70, Loss: 2.1578\n",
      "Iteration 80, Loss: 2.2883\n",
      "Iteration 90, Loss: 2.2986\n",
      "Iteration 0, Loss: 2.3182\n",
      "Iteration 10, Loss: 2.3122\n",
      "Iteration 20, Loss: 2.3069\n",
      "Iteration 30, Loss: 2.1952\n",
      "Iteration 40, Loss: 2.2127\n",
      "Iteration 50, Loss: 2.1222\n",
      "Iteration 60, Loss: 2.1702\n",
      "Iteration 70, Loss: 1.9801\n",
      "Iteration 80, Loss: 2.3629\n",
      "Iteration 90, Loss: 2.1082\n",
      "Iteration 0, Loss: 2.2682\n",
      "Iteration 10, Loss: 2.4280\n",
      "Iteration 20, Loss: 2.0627\n",
      "Iteration 30, Loss: 2.3173\n",
      "Iteration 40, Loss: 2.1847\n",
      "Iteration 50, Loss: 1.9595\n",
      "Iteration 60, Loss: 2.1168\n",
      "Iteration 70, Loss: 2.6220\n",
      "Iteration 80, Loss: 2.3864\n",
      "Iteration 90, Loss: 2.0026\n",
      "Iteration 0, Loss: 2.2533\n",
      "Iteration 10, Loss: 2.2523\n",
      "Iteration 20, Loss: 2.2518\n",
      "Iteration 30, Loss: 2.1097\n",
      "Iteration 40, Loss: 2.3499\n",
      "Iteration 50, Loss: 2.1187\n",
      "Iteration 60, Loss: 2.5913\n",
      "Iteration 70, Loss: 2.0713\n",
      "Iteration 80, Loss: 2.0556\n",
      "Iteration 90, Loss: 2.5155\n",
      "Iteration 0, Loss: 2.3615\n",
      "Iteration 10, Loss: 2.1828\n",
      "Iteration 20, Loss: 2.2963\n",
      "Iteration 30, Loss: 2.2407\n",
      "Iteration 40, Loss: 2.2503\n",
      "Iteration 50, Loss: 2.1833\n",
      "Iteration 60, Loss: 2.2541\n",
      "Iteration 70, Loss: 2.1969\n",
      "Iteration 80, Loss: 2.1767\n",
      "Iteration 90, Loss: 2.1339\n",
      "Iteration 0, Loss: 2.1462\n",
      "Iteration 10, Loss: 2.2074\n",
      "Iteration 20, Loss: 2.3145\n",
      "Iteration 30, Loss: 2.0872\n",
      "Iteration 40, Loss: 2.2224\n",
      "Iteration 50, Loss: 2.2184\n",
      "Iteration 60, Loss: 2.2182\n",
      "Iteration 70, Loss: 2.1414\n",
      "Iteration 80, Loss: 2.1341\n",
      "Iteration 90, Loss: 2.2545\n",
      "Iteration 0, Loss: 2.4576\n",
      "Iteration 10, Loss: 2.3223\n",
      "Iteration 20, Loss: 2.0808\n",
      "Iteration 30, Loss: 2.2425\n",
      "Iteration 40, Loss: 2.4213\n",
      "Iteration 50, Loss: 2.4055\n",
      "Iteration 60, Loss: 2.3913\n",
      "Iteration 70, Loss: 2.2057\n",
      "Iteration 80, Loss: 2.1772\n",
      "Iteration 90, Loss: 2.0919\n",
      "Iteration 0, Loss: 2.2416\n",
      "Iteration 10, Loss: 2.4919\n",
      "Iteration 20, Loss: 2.2289\n",
      "Iteration 30, Loss: 2.4087\n",
      "Iteration 40, Loss: 2.1863\n",
      "Iteration 50, Loss: 2.2862\n",
      "Iteration 60, Loss: 2.2050\n",
      "Iteration 70, Loss: 2.3535\n",
      "Iteration 80, Loss: 2.3440\n",
      "Iteration 90, Loss: 2.3354\n",
      "Iteration 0, Loss: 2.3911\n",
      "Iteration 10, Loss: 2.0680\n",
      "Iteration 20, Loss: 2.4119\n",
      "Iteration 30, Loss: 2.1095\n",
      "Iteration 40, Loss: 2.0085\n",
      "Iteration 50, Loss: 2.5091\n",
      "Iteration 60, Loss: 2.2628\n",
      "Iteration 70, Loss: 2.1791\n",
      "Iteration 80, Loss: 2.3194\n",
      "Iteration 90, Loss: 2.2857\n",
      "Iteration 0, Loss: 2.2828\n",
      "Iteration 10, Loss: 2.2802\n",
      "Iteration 20, Loss: 2.2906\n",
      "Iteration 30, Loss: 2.0953\n",
      "Iteration 40, Loss: 2.2270\n",
      "Iteration 50, Loss: 2.4129\n",
      "Iteration 60, Loss: 2.3544\n",
      "Iteration 70, Loss: 2.2654\n",
      "Iteration 80, Loss: 2.3741\n",
      "Iteration 90, Loss: 2.2254\n",
      "Iteration 0, Loss: 2.1563\n",
      "Iteration 10, Loss: 2.2707\n",
      "Iteration 20, Loss: 2.2691\n",
      "Iteration 30, Loss: 2.2389\n",
      "Iteration 40, Loss: 2.1470\n",
      "Iteration 50, Loss: 2.2996\n",
      "Iteration 60, Loss: 2.1019\n",
      "Iteration 70, Loss: 2.3310\n",
      "Iteration 80, Loss: 1.8106\n",
      "Iteration 90, Loss: 2.3211\n",
      "Iteration 0, Loss: 2.3085\n",
      "Iteration 10, Loss: 2.0960\n",
      "Iteration 20, Loss: 2.3304\n",
      "Iteration 30, Loss: 2.2972\n",
      "Iteration 40, Loss: 2.1842\n",
      "Iteration 50, Loss: 2.2175\n",
      "Iteration 60, Loss: 2.3427\n",
      "Iteration 70, Loss: 2.1358\n",
      "Iteration 80, Loss: 2.1465\n",
      "Iteration 90, Loss: 2.3020\n",
      "Iteration 0, Loss: 2.3908\n",
      "Iteration 10, Loss: 2.1996\n",
      "Iteration 20, Loss: 2.1761\n",
      "Iteration 30, Loss: 2.2569\n",
      "Iteration 40, Loss: 2.1454\n",
      "Iteration 50, Loss: 2.3924\n",
      "Iteration 60, Loss: 2.3791\n",
      "Iteration 70, Loss: 1.9709\n",
      "Iteration 80, Loss: 2.3019\n",
      "Iteration 90, Loss: 2.1099\n",
      "Iteration 0, Loss: 2.3411\n",
      "Iteration 10, Loss: 2.2620\n",
      "Iteration 20, Loss: 2.3219\n",
      "Iteration 30, Loss: 2.3640\n",
      "Iteration 40, Loss: 2.3656\n",
      "Iteration 50, Loss: 2.2513\n",
      "Iteration 60, Loss: 70.7892\n",
      "Iteration 70, Loss: 2.1291\n",
      "Iteration 80, Loss: 1.9607\n",
      "Iteration 90, Loss: 2.2437\n",
      "Iteration 0, Loss: 2.1585\n",
      "Iteration 10, Loss: 2.2376\n",
      "Iteration 20, Loss: 1.9372\n",
      "Iteration 30, Loss: 2.1649\n",
      "Iteration 40, Loss: 2.3431\n",
      "Iteration 50, Loss: 2.5689\n",
      "Iteration 60, Loss: 2.3626\n",
      "Iteration 70, Loss: 2.2553\n",
      "Iteration 80, Loss: 2.0291\n",
      "Iteration 90, Loss: 2.2822\n",
      "Iteration 0, Loss: 2.2465\n",
      "Iteration 10, Loss: 2.2442\n",
      "Iteration 20, Loss: 2.2426\n",
      "Iteration 30, Loss: 2.2229\n",
      "Iteration 40, Loss: 2.2893\n",
      "Iteration 50, Loss: 2.3076\n",
      "Iteration 60, Loss: 2.7352\n",
      "Iteration 70, Loss: 2.5278\n",
      "Iteration 80, Loss: 2.5625\n",
      "Iteration 90, Loss: 2.1792\n",
      "Iteration 0, Loss: 2.2487\n",
      "Iteration 10, Loss: 2.2448\n",
      "Iteration 20, Loss: 2.2770\n",
      "Iteration 30, Loss: 2.2416\n",
      "Iteration 40, Loss: 2.2421\n",
      "Iteration 50, Loss: 2.2427\n",
      "Iteration 60, Loss: 2.3435\n",
      "Iteration 70, Loss: 2.0213\n",
      "Iteration 80, Loss: 2.2363\n",
      "Iteration 90, Loss: 2.2913\n",
      "Iteration 0, Loss: 2.3077\n",
      "Iteration 10, Loss: 2.1815\n",
      "Iteration 20, Loss: 2.3736\n",
      "Iteration 30, Loss: 2.3610\n",
      "Iteration 40, Loss: 2.1923\n",
      "Iteration 50, Loss: 2.2407\n",
      "Iteration 60, Loss: 2.3090\n",
      "Iteration 70, Loss: 2.3038\n",
      "Iteration 80, Loss: 2.2991\n",
      "Iteration 90, Loss: 2.3731\n",
      "Iteration 0, Loss: 2.3234\n",
      "Iteration 10, Loss: 2.2816\n",
      "Iteration 20, Loss: 2.3104\n",
      "Iteration 30, Loss: 2.2034\n",
      "Iteration 40, Loss: 2.1177\n",
      "Iteration 50, Loss: 2.4550\n",
      "Iteration 60, Loss: 2.4408\n",
      "Iteration 70, Loss: 2.2331\n",
      "Iteration 80, Loss: 2.3374\n",
      "Iteration 90, Loss: 2.1048\n",
      "Iteration 0, Loss: 2.2445\n",
      "Iteration 10, Loss: 2.0688\n",
      "Iteration 20, Loss: 2.2557\n",
      "Iteration 30, Loss: 2.3200\n",
      "Iteration 40, Loss: 2.6643\n",
      "Iteration 50, Loss: 2.0956\n",
      "Iteration 60, Loss: 2.4022\n",
      "Iteration 70, Loss: 2.1140\n",
      "Iteration 80, Loss: 2.1265\n",
      "Iteration 90, Loss: 2.1379\n",
      "Iteration 0, Loss: 2.1891\n",
      "Iteration 10, Loss: 2.2747\n",
      "Iteration 20, Loss: 2.2012\n",
      "Iteration 30, Loss: 2.3477\n",
      "Iteration 40, Loss: 2.1139\n",
      "Iteration 50, Loss: 2.1623\n",
      "Iteration 60, Loss: 2.2241\n",
      "Iteration 70, Loss: 2.3879\n",
      "Iteration 80, Loss: 2.2193\n",
      "Iteration 90, Loss: 2.2995\n",
      "Iteration 0, Loss: 2.2422\n",
      "Iteration 10, Loss: 8.5796\n",
      "Iteration 20, Loss: 2.1588\n",
      "Iteration 30, Loss: 2.3879\n",
      "Iteration 40, Loss: 2.4473\n",
      "Iteration 50, Loss: 2.2726\n",
      "Iteration 60, Loss: 2.2825\n",
      "Iteration 70, Loss: 2.2902\n",
      "Iteration 80, Loss: 2.0944\n",
      "Iteration 90, Loss: 2.2786\n",
      "Iteration 0, Loss: 2.4346\n",
      "Iteration 10, Loss: 2.2832\n",
      "Iteration 20, Loss: 2.2802\n",
      "Iteration 30, Loss: 2.2776\n",
      "Iteration 40, Loss: 2.3445\n",
      "Iteration 50, Loss: 2.3682\n",
      "Iteration 60, Loss: 2.3554\n",
      "Iteration 70, Loss: 2.2799\n",
      "Iteration 80, Loss: 2.1524\n",
      "Iteration 90, Loss: 2.3329\n",
      "Iteration 0, Loss: 2.1838\n",
      "Iteration 10, Loss: 2.3268\n",
      "Iteration 20, Loss: 2.3195\n",
      "Iteration 30, Loss: 2.2621\n",
      "Iteration 40, Loss: 2.1403\n",
      "Iteration 50, Loss: 2.0418\n",
      "Iteration 60, Loss: 2.3508\n",
      "Iteration 70, Loss: 2.3883\n",
      "Iteration 80, Loss: 2.2110\n",
      "Iteration 90, Loss: 2.2948\n",
      "Iteration 0, Loss: 2.1284\n",
      "Iteration 10, Loss: 2.3201\n",
      "Iteration 20, Loss: 2.1470\n",
      "Iteration 30, Loss: 2.2882\n",
      "Iteration 40, Loss: 2.2846\n",
      "Iteration 50, Loss: 2.2814\n",
      "Iteration 60, Loss: 2.1834\n",
      "Iteration 70, Loss: 2.0813\n",
      "Iteration 80, Loss: 2.3693\n",
      "Iteration 90, Loss: 2.2737\n",
      "Iteration 0, Loss: 2.3473\n",
      "Iteration 10, Loss: 2.3539\n",
      "Iteration 20, Loss: 2.2488\n",
      "Iteration 30, Loss: 2.2498\n",
      "Iteration 40, Loss: 2.3127\n",
      "Iteration 50, Loss: 2.2459\n",
      "Iteration 60, Loss: 2.2900\n",
      "Iteration 70, Loss: 2.2533\n",
      "Iteration 80, Loss: 2.3780\n",
      "Iteration 90, Loss: 2.1855\n",
      "Iteration 0, Loss: 2.0049\n",
      "Iteration 10, Loss: 2.5695\n",
      "Iteration 20, Loss: 1.9581\n",
      "Iteration 30, Loss: 2.2720\n",
      "Iteration 40, Loss: 2.1745\n",
      "Iteration 50, Loss: 2.1796\n",
      "Iteration 60, Loss: 2.1847\n",
      "Iteration 70, Loss: 2.2013\n",
      "Iteration 80, Loss: 2.0276\n",
      "Iteration 90, Loss: 2.0469\n",
      "Iteration 0, Loss: 2.4231\n",
      "Iteration 10, Loss: 2.2886\n",
      "Iteration 20, Loss: 2.1075\n",
      "Iteration 30, Loss: 2.2493\n",
      "Iteration 40, Loss: 2.3591\n",
      "Iteration 50, Loss: 2.2895\n",
      "Iteration 60, Loss: 2.2860\n",
      "Iteration 70, Loss: 2.2828\n",
      "Iteration 80, Loss: 2.2799\n",
      "Iteration 90, Loss: 2.0023\n",
      "Iteration 0, Loss: 1.9354\n",
      "Iteration 10, Loss: 2.3990\n",
      "Iteration 20, Loss: 2.3649\n",
      "Iteration 30, Loss: 2.3526\n",
      "Iteration 40, Loss: 2.3419\n",
      "Iteration 50, Loss: 2.2304\n",
      "Iteration 60, Loss: 2.3100\n",
      "Iteration 70, Loss: 2.3129\n",
      "Iteration 80, Loss: 2.3056\n",
      "Iteration 90, Loss: 2.2910\n",
      "Iteration 0, Loss: 2.0898\n",
      "Iteration 10, Loss: 2.2936\n",
      "Iteration 20, Loss: 2.2439\n",
      "Iteration 30, Loss: 2.1948\n",
      "Iteration 40, Loss: 2.2160\n",
      "Iteration 50, Loss: 2.2959\n",
      "Iteration 60, Loss: 2.1687\n",
      "Iteration 70, Loss: 2.5289\n",
      "Iteration 80, Loss: 2.0917\n",
      "Iteration 90, Loss: 2.1731\n",
      "Iteration 0, Loss: 2.1339\n",
      "Iteration 10, Loss: 2.3851\n",
      "Iteration 20, Loss: 2.1807\n",
      "Iteration 30, Loss: 2.2677\n",
      "Iteration 40, Loss: 2.2151\n",
      "Iteration 50, Loss: 2.2881\n",
      "Iteration 60, Loss: 2.1407\n",
      "Iteration 70, Loss: 2.1620\n",
      "Iteration 80, Loss: 2.2384\n",
      "Iteration 90, Loss: 2.1813\n",
      "Iteration 0, Loss: 2.1031\n",
      "Iteration 10, Loss: 2.3767\n",
      "Iteration 20, Loss: 2.1994\n",
      "Iteration 30, Loss: 2.0383\n",
      "Iteration 40, Loss: 2.3478\n",
      "Iteration 50, Loss: 2.3745\n",
      "Iteration 60, Loss: 2.3621\n",
      "Iteration 70, Loss: 2.3511\n",
      "Iteration 80, Loss: 2.1517\n",
      "Iteration 90, Loss: 2.3359\n",
      "Iteration 0, Loss: 2.2829\n",
      "Iteration 10, Loss: 2.2639\n",
      "Iteration 20, Loss: 2.2772\n",
      "Iteration 30, Loss: 2.3296\n",
      "Iteration 40, Loss: 2.0657\n",
      "Iteration 50, Loss: 2.3097\n",
      "Iteration 60, Loss: 2.3025\n",
      "Iteration 70, Loss: 2.2420\n",
      "Iteration 80, Loss: 2.1126\n",
      "Iteration 90, Loss: 2.4899\n",
      "Iteration 0, Loss: 2.0701\n",
      "Iteration 10, Loss: 2.2782\n",
      "Iteration 20, Loss: 2.1894\n",
      "Iteration 30, Loss: 2.6084\n",
      "Iteration 40, Loss: 2.4354\n",
      "Iteration 50, Loss: 2.2248\n",
      "Iteration 60, Loss: 2.3436\n",
      "Iteration 70, Loss: 2.3905\n",
      "Iteration 80, Loss: 2.3778\n",
      "Iteration 90, Loss: 2.3663\n",
      "Iteration 0, Loss: 2.3560\n",
      "Iteration 10, Loss: 2.1804\n",
      "Iteration 20, Loss: 2.1258\n",
      "Iteration 30, Loss: 2.5024\n",
      "Iteration 40, Loss: 2.0680\n",
      "Iteration 50, Loss: 2.4274\n",
      "Iteration 60, Loss: 2.4022\n",
      "Iteration 70, Loss: 2.3533\n",
      "Iteration 80, Loss: 2.3000\n",
      "Iteration 90, Loss: 2.2549\n",
      "Iteration 0, Loss: 2.3571\n",
      "Iteration 10, Loss: 2.3477\n",
      "Iteration 20, Loss: 2.3392\n",
      "Iteration 30, Loss: 2.3324\n",
      "Iteration 40, Loss: 2.3745\n",
      "Iteration 50, Loss: 2.4307\n",
      "Iteration 60, Loss: 2.1725\n",
      "Iteration 70, Loss: 2.4149\n",
      "Iteration 80, Loss: 2.3950\n",
      "Iteration 90, Loss: 2.1380\n",
      "Iteration 0, Loss: 2.1651\n",
      "Iteration 10, Loss: 2.2270\n",
      "Iteration 20, Loss: 2.1849\n",
      "Iteration 30, Loss: 2.2629\n",
      "Iteration 40, Loss: 2.5219\n",
      "Iteration 50, Loss: 2.1814\n",
      "Iteration 60, Loss: 2.1834\n",
      "Iteration 70, Loss: 2.1781\n",
      "Iteration 80, Loss: 2.2298\n",
      "Iteration 90, Loss: 2.4205\n",
      "Iteration 0, Loss: 2.3224\n",
      "Iteration 10, Loss: 2.3006\n",
      "Iteration 20, Loss: 2.4596\n",
      "Iteration 30, Loss: 1.9565\n",
      "Iteration 40, Loss: 2.2315\n",
      "Iteration 50, Loss: 2.1472\n",
      "Iteration 60, Loss: 2.2644\n",
      "Iteration 70, Loss: 2.2600\n",
      "Iteration 80, Loss: 2.2567\n",
      "Iteration 90, Loss: 2.0367\n",
      "Iteration 0, Loss: 2.2784\n",
      "Iteration 10, Loss: 2.3674\n",
      "Iteration 20, Loss: 2.0920\n",
      "Iteration 30, Loss: 2.1606\n",
      "Iteration 40, Loss: 2.0562\n",
      "Iteration 50, Loss: 2.2566\n",
      "Iteration 60, Loss: 2.4132\n",
      "Iteration 70, Loss: 2.3136\n",
      "Iteration 80, Loss: 2.1344\n",
      "Iteration 90, Loss: 2.1455\n",
      "Iteration 0, Loss: 2.1557\n",
      "Iteration 10, Loss: 2.1495\n",
      "Iteration 20, Loss: 2.3504\n",
      "Iteration 30, Loss: 2.1584\n",
      "Iteration 40, Loss: 2.2726\n",
      "Iteration 50, Loss: 2.2810\n",
      "Iteration 60, Loss: 2.6093\n",
      "Iteration 70, Loss: 2.2048\n",
      "Iteration 80, Loss: 2.3636\n",
      "Iteration 90, Loss: 2.4292\n",
      "Iteration 0, Loss: 2.0444\n",
      "Iteration 10, Loss: 2.1727\n",
      "Iteration 20, Loss: 2.5556\n",
      "Iteration 30, Loss: 2.1404\n",
      "Iteration 40, Loss: 1.9695\n",
      "Iteration 50, Loss: 2.3822\n",
      "Iteration 60, Loss: 2.2581\n",
      "Iteration 70, Loss: 2.2862\n",
      "Iteration 80, Loss: 2.2825\n",
      "Iteration 90, Loss: 2.2792\n",
      "Iteration 0, Loss: 2.1567\n",
      "Iteration 10, Loss: 2.2967\n",
      "Iteration 20, Loss: 2.1943\n",
      "Iteration 30, Loss: 2.3399\n",
      "Iteration 40, Loss: 2.0710\n",
      "Iteration 50, Loss: 2.2067\n",
      "Iteration 60, Loss: 2.3396\n",
      "Iteration 70, Loss: 2.4065\n",
      "Iteration 80, Loss: 2.1923\n",
      "Iteration 90, Loss: 2.1038\n",
      "Iteration 0, Loss: 2.1187\n",
      "Iteration 10, Loss: 2.1320\n",
      "Iteration 20, Loss: 2.0804\n",
      "Iteration 30, Loss: 2.3349\n",
      "Iteration 40, Loss: 2.2031\n",
      "Iteration 50, Loss: 2.1822\n",
      "Iteration 60, Loss: 2.1923\n",
      "Iteration 70, Loss: 2.2246\n",
      "Iteration 80, Loss: 2.1608\n",
      "Iteration 90, Loss: 2.2728\n",
      "Iteration 0, Loss: 2.2693\n",
      "Iteration 10, Loss: 2.2828\n",
      "Iteration 20, Loss: 2.2794\n",
      "Iteration 30, Loss: 2.2399\n",
      "Iteration 40, Loss: 2.1501\n",
      "Iteration 50, Loss: 2.0772\n",
      "Iteration 60, Loss: 2.0122\n",
      "Iteration 70, Loss: 2.0958\n",
      "Iteration 80, Loss: 2.1081\n",
      "Iteration 90, Loss: 2.1484\n",
      "Iteration 0, Loss: 2.3822\n",
      "Iteration 10, Loss: 2.2942\n",
      "Iteration 20, Loss: 2.2889\n",
      "Iteration 30, Loss: 2.2844\n",
      "Iteration 40, Loss: 2.2910\n",
      "Iteration 50, Loss: 2.2246\n",
      "Iteration 60, Loss: 2.2076\n",
      "Iteration 70, Loss: 2.3138\n",
      "Iteration 80, Loss: 2.2574\n",
      "Iteration 90, Loss: 2.2365\n",
      "Iteration 0, Loss: 2.2658\n",
      "Iteration 10, Loss: 2.0425\n",
      "Iteration 20, Loss: 2.2924\n",
      "Iteration 30, Loss: 2.4411\n",
      "Iteration 40, Loss: 2.1559\n",
      "Iteration 50, Loss: 2.2912\n",
      "Iteration 60, Loss: 2.2710\n",
      "Iteration 70, Loss: 2.3961\n",
      "Iteration 80, Loss: 2.0877\n",
      "Iteration 90, Loss: inf\n",
      "Iteration 0, Loss: 2.1828\n",
      "Iteration 10, Loss: 2.1849\n",
      "Iteration 20, Loss: 2.0987\n",
      "Iteration 30, Loss: 2.3106\n",
      "Iteration 40, Loss: 2.3683\n",
      "Iteration 50, Loss: 2.0392\n",
      "Iteration 60, Loss: -0.0000\n",
      "Iteration 70, Loss: 2.6749\n",
      "Iteration 80, Loss: 2.2959\n",
      "Iteration 90, Loss: 1.9929\n",
      "[[-0.13011767 -0.07364311 -0.53745795 -0.13228532  0.02430842  0.56998975\n",
      "  -0.05705152 -0.3562023  -0.11334632 -0.50727588 -0.04712649 -0.07291334\n",
      "  -0.06799789 -0.30797123 -0.02796951 -0.02168001 -0.0703477  -0.19576718\n",
      "  -0.0451518  -0.03695573 -0.04483675 -0.04052547 -0.04015499 -0.03301704\n",
      "  -0.10964216]]\n",
      "above is the updated W1 and below is the he initialised\n",
      "[[-0.13011767 -0.07364311 -0.53745795 -0.13228532  0.02430842  0.56998975\n",
      "  -0.05705152 -0.3562023  -0.11334632 -0.50727588 -0.04712649 -0.07291334\n",
      "  -0.06799789 -0.30797123 -0.02796951 -0.02168001 -0.0703477  -0.19576718\n",
      "  -0.0451518  -0.03695573 -0.04483675 -0.04052547 -0.04015499 -0.03301704\n",
      "  -0.10964216]]\n"
     ]
    }
   ],
   "source": [
    "W_global_3 = W3\n",
    "W_global_2 = W2\n",
    "W_global_1 = W1\n",
    "\n",
    "b_global_3 = b3\n",
    "b_global_2 = b2\n",
    "b_global_1 = b1\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "W3_final, W2_final, W1_final, b3_final,b2_final,b1_final = grad_desc_datafeeder(train_data_new, labels_col,W_global_3,W_global_2, W_global_1, b_global_3,b_global_2,b_global_1,learning_rate)\n",
    "print(b1_final)\n",
    "print(\"above is the updated W1 and below is the he initialised\")\n",
    "print(b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "6d3fd017-5ab5-4cae-8c2b-9de79e9d01b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.13011767 -0.07364311 -0.53745795 -0.13228532  0.02430842  0.56998975\n",
      "  -0.05705152 -0.3562023  -0.11334632 -0.50727588 -0.04712649 -0.07291334\n",
      "  -0.06799789 -0.30797123 -0.02796951 -0.02168001 -0.0703477  -0.19576718\n",
      "  -0.0451518  -0.03695573 -0.04483675 -0.04052547 -0.04015499 -0.03301704\n",
      "  -0.10964216]]\n",
      "--------------------\n",
      "[[-0.13011767 -0.07364311 -0.53745795 -0.13228532  0.02430842  0.56998975\n",
      "  -0.05705152 -0.3562023  -0.11334632 -0.50727588 -0.04712649 -0.07291334\n",
      "  -0.06799789 -0.30797123 -0.02796951 -0.02168001 -0.0703477  -0.19576718\n",
      "  -0.0451518  -0.03695573 -0.04483675 -0.04052547 -0.04015499 -0.03301704\n",
      "  -0.10964216]]\n"
     ]
    }
   ],
   "source": [
    "print(b_global_1)\n",
    "print(\"--------------------\")\n",
    "print(b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e7b67b18-f385-48ae-a243-e21385a9d6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "print(labels_col.shape)\n",
    "print(labels_col.iloc[255])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6314387d-9316-4fa8-b84f-cd22f1fe082e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.0794415416798357\n"
     ]
    }
   ],
   "source": [
    "my_array = [1,10,8,6,7,4,65,12]\n",
    "my_variable = my_array[2]\n",
    "print(-np.log(my_variable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da0d0a9-210e-4235-a874-df1bdee105cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
